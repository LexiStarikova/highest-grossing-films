{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1**: Scrape the Wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Fetch Wikipedia page\n",
    "def fetch_wikipedia_page() -> BeautifulSoup:\n",
    "    url: str = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n",
    "    response: requests.Response = requests.get(url)\n",
    "    soup: BeautifulSoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    return soup\n",
    "\n",
    "soup = fetch_wikipedia_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2**: Extract relevant data available within that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_digits(value: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts only numeric digits from a box office string.\n",
    "\n",
    "    Args:\n",
    "        value (str): The box office revenue string.\n",
    "\n",
    "    Returns:\n",
    "        str: A cleaned string containing only digits.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\D', '', value)  # Remove all non-digit characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from bs4.element import Tag\n",
    "\n",
    "# Step 2: Extract data\n",
    "def extract_main_data(soup: BeautifulSoup) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extracts key movie data from a Wikipedia table.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): A BeautifulSoup object containing the parsed HTML.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of dictionaries, each containing movie details:\n",
    "            - \"title\" (str): The movie title.\n",
    "            - \"release_year\" (int): The year the movie was released.\n",
    "            - \"box_office\" (int): The box office revenue as a cleaned string.\n",
    "            - \"film_url\" (str): The Wikipedia URL of the film.\n",
    "    \"\"\"\n",
    "    table: Tag = soup.find(\"table\", {\"class\": \"wikitable\"})  # Locate the main table\n",
    "\n",
    "    data: List[Dict[str, Any]] = []\n",
    "    base_url: str = \"https://en.wikipedia.org\"\n",
    "\n",
    "    for row in table.find_all(\"tr\")[1:]:  # Skip header row\n",
    "        columns: List[Tag] = row.find_all([\"th\", \"td\"])  # Extract table cells\n",
    "        \n",
    "        # Extract movie title and link\n",
    "        title_tag: Tag = columns[2].find(\"a\")\n",
    "        title: str = title_tag.text.strip() if title_tag else columns[2].text.strip()\n",
    "        film_url: str = base_url + title_tag[\"href\"].replace(' ', '_') if title_tag else None\n",
    "\n",
    "        # Extract release year\n",
    "        release_year: int = int(columns[4].text.strip())\n",
    "\n",
    "        # Extract and clean box office revenue (remove \"$\" and \",\")\n",
    "        box_office: int = int(extract_digits(columns[3].text.strip().replace(\"$\", \"\").replace(\",\", \"\")))\n",
    "        \n",
    "        # Store extracted data\n",
    "        data.append({\n",
    "            \"title\": title,\n",
    "            \"release_year\": release_year,\n",
    "            \"box_office\": box_office,\n",
    "            \"film_url\": film_url\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "data: List[Dict[str, Any]] = extract_main_data(soup)\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3**: Extract additional information (directors, countries, production companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def extract_production_companies(soup: BeautifulSoup) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts the list of production companies from a Wikipedia infobox.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): A BeautifulSoup object containing the parsed HTML.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of production company names.\n",
    "    \"\"\"\n",
    "    production_element: Optional[Tag] = None\n",
    "\n",
    "    # Search for the \"Production company\" or similar label in the infobox\n",
    "    for th in soup.find_all(\"th\", class_=\"infobox-label\"):\n",
    "        if th.find(\"div\"):  # Companies are wrapped in a <div>\n",
    "            if th.get_text().strip().startswith(\"Productioncompan\"):  # Handles variations like \"Production company\"\n",
    "                production_element = th\n",
    "                break\n",
    "\n",
    "    if production_element is None:\n",
    "        return []  # Return an empty list if no production company section is found\n",
    "\n",
    "    # Find the corresponding <td> element that contains the company names\n",
    "    production_td: Optional[Tag] = production_element.find_next_sibling(\"td\")\n",
    "\n",
    "    if production_td is None:\n",
    "        return []  # Handle cases where <td> is missing\n",
    "\n",
    "    # Extract production companies by filtering only valid Wikipedia links\n",
    "    companies: List[str] = [\n",
    "        a.text.strip() for a in production_td.find_all(\"a\")\n",
    "        if a.has_attr(\"href\") and a[\"href\"].startswith(\"/wiki/\")\n",
    "    ]\n",
    "\n",
    "    return companies\n",
    "\n",
    "# companies = extract_production_companies(soup)\n",
    "# print(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clear_and_split_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Cleans and splits a text string by removing Wikipedia-style references \n",
    "    (e.g., [1], [2]) and excessive newlines.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to clean and split.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of cleaned substrings split by newline characters.\n",
    "    \"\"\"\n",
    "    # Remove Wikipedia-style references like [1], [2], etc.\n",
    "    text = re.sub(r'\\[\\d+\\]', '\\n', text)\n",
    "\n",
    "    # Replace multiple consecutive newlines (\\n+) with a single newline\n",
    "    text = re.sub(r'\\n+', '\\n', text).strip()\n",
    "\n",
    "    # Split text into a list using newline as a separator\n",
    "    return text.split('\\n')\n",
    "\n",
    "# cleaned_list = clear_and_split_text(\"United States[2]\\nUnited Kingdom[3]\\n\\nCanada[4]\")\n",
    "# print(cleaned_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from random import uniform\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def fetch_film_details(film: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetches additional details (directors, countries, production companies) \n",
    "    from an individual Wikipedia film page.\n",
    "\n",
    "    Args:\n",
    "        film (Dict[str, Any]): A dictionary containing basic film details, \n",
    "                               including a 'film_url' key.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The updated film dictionary with extracted details.\n",
    "    \"\"\"\n",
    "    if not film.get(\"film_url\"):\n",
    "        return film  # Skip if no Wikipedia URL is available\n",
    "\n",
    "    # Send request to fetch the Wikipedia page\n",
    "    response: requests.Response = requests.get(film[\"film_url\"])\n",
    "    soup: BeautifulSoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract directors\n",
    "    director_element: Tag = soup.find(\"th\", string=\"Directed by\")\n",
    "    if director_element:\n",
    "        directors_raw: str = director_element.find_next_sibling(\"td\").text.strip()\n",
    "        film[\"directors\"] = clear_and_split_text(directors_raw)\n",
    "\n",
    "    # Extract countries of origin\n",
    "    country_element: Tag = soup.find(\"th\", string=lambda text: text and text.startswith(\"Countr\"))  # Matches \"Country\" or \"Countries\"\n",
    "    if country_element:\n",
    "        countries_raw: str = country_element.find_next_sibling(\"td\").text.strip()\n",
    "        film[\"countries_of_origin\"] = clear_and_split_text(countries_raw)\n",
    "\n",
    "    # Extract production companies\n",
    "    film[\"production_companies\"] = extract_production_companies(soup)\n",
    "\n",
    "    return film\n",
    "\n",
    "# Process each film with a delay to avoid being blocked\n",
    "for film in tqdm(data, desc=\"Fetching film details\"):\n",
    "    film = fetch_film_details(film)\n",
    "    sleep(uniform(3, 10))  # Wait 3-10 seconds between requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of data extracted\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4**: Save data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save to a file\n",
    "with open(\"films.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)  # Pretty-print with indentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load data\n",
    "with open(\"films.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5**: Analyze the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Create histogram\n",
    "df['release_year'].hist(\n",
    "    bins=[y for y in range(df['release_year'].min(), df['release_year'].max() + 5, 5)], \n",
    "    color='skyblue', edgecolor='black', alpha=0.7\n",
    ")\n",
    "\n",
    "# Titles and labels\n",
    "plt.title('Release Year Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Release Year', fontsize=12)\n",
    "plt.ylabel('Number of Films', fontsize=12)\n",
    "\n",
    "# Add grid for readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Create histogram\n",
    "plt.hist(np.log(df['box_office']), color='salmon', edgecolor='black', alpha=0.75)\n",
    "\n",
    "# Titles and labels\n",
    "plt.title('Box Office Revenue Distribution (Log-Transformed)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Log(Box Office Revenue) (in billions)', fontsize=12)\n",
    "plt.ylabel('Number of Films', fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: The data containes a wrongly parsed value for one film. This value is adjusted manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"box_office\", ascending=False).head(2)[[\"title\", \"box_office\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in data:\n",
    "    if row['title'] == 'The Fate of the Furious':\n",
    "        row['box_office'] = 1238764765\n",
    "        print(\"Fixed the value!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "categories = ['Multiple Countries of Origin', 'Multiple Directors', 'Multiple Production Companies']\n",
    "counts = [\n",
    "    len(df[df['countries_of_origin'].apply(len) > 1]),\n",
    "    len(df[df['directors'].apply(len) > 1]),\n",
    "    len(df[df['production_companies'].apply(len) > 1])\n",
    "]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.bar(categories, counts, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Titles and labels\n",
    "plt.title('Movies with Multiple Attributes', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Category', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "\n",
    "# Display values on top of bars\n",
    "for i, count in enumerate(counts):\n",
    "    plt.text(i, count + 1, str(count), ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Step 6***: Create a database using `sqlite3` and insert the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to SQLite database (or create it if it doesn't exist)\n",
    "conn = sqlite3.connect(\"films.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS films (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    title TEXT NOT NULL,\n",
    "    release_year INTEGER,\n",
    "    director TEXT,\n",
    "    box_office INTEGER,\n",
    "    country TEXT,\n",
    "    production_companies TEXT\n",
    ");\n",
    "''')\n",
    "\n",
    "# Insert data into table\n",
    "for film in data:\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO films (title, release_year, director, box_office, country, production_companies)\n",
    "        VALUES (?, ?, ?, ?, ?, ?)\n",
    "        \"\"\",\n",
    "        (\n",
    "            film[\"title\"],\n",
    "            film.get(\"release_year\"),\n",
    "            \", \".join(film.get(\"directors\", [])),\n",
    "            film.get(\"box_office\"),\n",
    "            \", \".join(film.get(\"countries_of_origin\", [])),\n",
    "            \", \".join(film.get(\"production_companies\", []))\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Commit and close connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"films.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch and display data\n",
    "cursor.execute(\"SELECT * FROM films\")\n",
    "rows = cursor.fetchall()\n",
    "for row in rows[:5]:\n",
    "    print(row)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 8**: Adjust the JSON fields with respect to requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for film in data:\n",
    "    for column in ['directors', 'countries_of_origin', 'production_companies']:\n",
    "        film[column] = \", \".join(film[column])\n",
    "\n",
    "# Save to a file\n",
    "with open(\"films.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)  # Pretty-print with indentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
